# 海量数据处理与挖掘

## 1  Hadoop海量数据实现原理

### 1.1 单点结构

单点结构：也就是一台计算机，对于大数据，高并发来讲，我们所用的计算机的性能是远远不够的。而且单台及计算机对于我们数据来讲也是十分不安全的，当我们计算机发生损坏时，我们的数据就全都没有了，在我们做一些练习时可以使用，在真正的业务中是无法使用的。

![1574933675878](image\1574933675878.png)

###  1.2 集群架构以及面临的问题

#### 1.2.1 集群架构

集群技术是一种较新的技术，通过集群技术，可以在付出较低成本的情况下获得在性能、可靠性、灵活性方面的相对较高的收益，其任务调度则是集群系统中的核心技术。

集群是一组相互独立的、通过高速网络互联的计算机，它们构成了一个组，并以单一系统的模式加以管理。一个客户与集群相互作用时，集群像是一个独立的服务器。集群配置是用于提高可用性和可缩放性。

- 高性能

- 降低成本

- 提高可扩展性

- 增强可靠性

- 每一个rack（机架服务器）包含16到64个Linux节点
- 2011年据统计Google约有100万台机器



![1574992376028](image\1574992376028.png)

#### 1.2.2 面临问题

- 节点故障
  - 1000台服务器的集群=》平均故障率 1次/天
  - 100万台服务器的集群=》平均故障率1000次/天

- 如何保持数据的持续性
  - 某些节点故障的情形下不影响数据的使用
  - 长时间集群的运算，如何应对节点的故障
  - 所拥有的所有资源将故障转移到服务器群集中的其他节点

- 网络带宽瓶颈
  - 网络带宽 = 1 Gbps
  - 移动10TB数据需要花费将近一天

- 分布式编程非常复杂
  - 需要一个简单的模型能够隐去所有的复杂性

###  1.3 Hadoop集群架构

#### 1.3.1 Hadoop简介

Hadoop是一个分布式系统基础架构，一套行业大数据标准开源软件，在分布式环境下提供了海量数据的存储和计算。运行于一般的商用服务器上，具有高容错、高可靠性、高扩展性等特点，特别适合写一次，读多次的场景。

- ##### 架构核心

其框架最核心的设计是HDFS、Map Reduce和YARN。

![1574995543362](image\1574995543362.png)

- HDFS: 分布式文件存储
- YARN:分布式资源管理
- Map Reduce: 分布式计算
- Others: 利用YARN的资源管理功能实现其他的数据处理方式

#### 1.3.2 Hadoop 分布式集群

- hadoop是依据Map Reduce的原理，用Java语言实现的分布式处理机制
- Hadoop是一个能够对大量数据进行分布式处理的软件框架，实现了Google的MapReduce编程模型和框架，能够把应用程序分割成许多的小的工作单元，并把这些单元放到任何集群节点上执行
- Map Reduce是Hadoop中的一个数据运算核心模块
- Map Reduce是一种模式，一种云计算的核心计算模式，一种分布式运算技术，也是简化的分布式编程模式，它主要用于解决问题的程序开发模型，也是开发人员拆解问题的方法。

#### 1.3.3 Map Reduce集群运算问题的解决方案

- 在多节点上冗余地存储数据，以保证数据的持续性
- 将计算移向数据端，以最大程度减少数据移动
- 简单的程序模型，隐藏所有的复杂度

###  1.4 冗余化数据存储

数据冗余是指数据之间的重复，也可以说是同一数据存储在不同数据文件中的现象。可以说增加数据的独立性和减少数据冗余是企业范围信息资源管理和大规模信息系统获得成功的前提条件。

- 分布式文件存储系统
  - 提供全局的文件命名空间，冗余度和可获取性
  - 例如Google的GFS，hadoop的HDFS

- 典型的应用场景与模式
  - 超大级别的数据量（100GB到100TB级别）
  - 数据很少被全部替换
  - 最常见的操作为读取和追加数据

###  1.5 分布式文件系统

分布式文件系统是指文件系统管理的物理存储资源不一定直接连接在本地节点上，而是通过计算机网络与节点【可简单的理解为一台计算机】相连。分布式文件系统的设计基于客户机/服务器模式。一个典型的网络可能包括多个供多用户访问的服务器。另外，对等特性允许一些系统扮演客户机和服务器的双重角色。

- 数据以‘块状’形式在多台机器上存储
- 每个数据块都会重复地在多台机器上存储
- 保证数据的持续性和随时可取性

- 服务器块同时也用作计算服务器
- 把运算挪向数据处

![1574997338559](image\1574997338559.png)

- 服务器块
  - 文件被分作16-64MB大小的连续块
  - 每个文件块会被重复的存储2到3次
  - 尽量保证重复的数据块在不同的机架上

- 主节点
  - Hadoop的HDFS里叫做Name节点
  - 存储元数据记录文件存储结构和地址
  - 最常见的操作为读取和追加数据

- 文件访问的客户端库
  - 询问主节点以获取块服务器地址
  - 直接连接相应服务器块获取数据

## 2 Map Reduce

### 2.1 Map Reduce 简介

 Map Reduce 借鉴了函数式程序设计语言的设计思想，其软件实现是指定一个Map 函数，把键值对(key/value)映射成新的键值对(key/value)，形成一系列中间结果形式key/value 对，然后把它们传给Reduce(规约)函数，把具有相同中间形式key 的value 合并在一起。Map 和Reduce 函数具有一定的关联性。

 Map Reduce致力于解决大规模数据处理的问题，分布式处理策略，计算模型，对海量数据处理。

Map Reduce技术特征总结：

- 自动并行化：系统自动进行作业并行化处理；

- 自动可靠处理：系统自动处理节点/任务的故障检测和恢复；

- 灵活扩展：节点可以灵活加入和退出，系统自动感知节点状态并进行处理；

- 高性能：计算任务将被调度至数据所在的节点，减少网络开销，提升执行性能。

Map Reduce 应用场景：

- 从web服务器日志中找出高频热门url
- 搜索词统计
- 区分垃圾邮件和短信
- 舆情分析（正负面评论）

### 2.2 Map Reduce 工作流程

- Map
  - 逐个文件逐行扫描
  - 扫描的同时抽取出我们感兴趣的内容（Keys）
- Group by key
  - 排序和洗牌
- Reduce
  - 聚合，总结，过滤或转换
  - 写入结果

![1575010144752](image\1575010144752.png)



- 文件准备

- Map过程：接受一个键值对，产生一组键值对，比如（Deer，1），代表Deer为键，1为值，代表Deer单词的数量

- 派发过程：Shuffle将键值对派发给Reduce

- Reduce过程：将相同键的值累加（计算靠近的数据）

- 输出计算结果

Map Reduce主要是先读取文件数据，然后进行Map处理，接着Reduce处理，最后把处理结果写到文件中。Map和Reduce函数要根据具体问题具体实现。

Map Reduce集群由普通PC机构成，为无共享式架构。在处理之前，将数据集分布至各个节点。处理时，每个节点就近读取本地存储的数据处理（map），将处理后的数据进行合并（combine）、排序（shuffle and  sort）后再分发（至reduce节点），避免了大量数据的传输，提高了处理效率。无共享式架构的另一个好处是配合复制（replication）策略，集群可以具有良好的容错性，一部分节点的down机对集群的正常工作不会造成影响。

### 2.3 Map步骤

- 每个输入分片会让一个map任务来处理，默认情况下，以HDFS的一个块的大小（默认为64M，可设置）为一个分片。map输出的结果会暂时放在一个环形内存缓冲区中（该缓冲区的大小默认为100M）。当该缓冲区快要溢出时，会在本地文件系统中创建一个溢出文件，将该缓冲区中的数据写入这个文件。

- 在写入磁盘之前，线程首先根据reduce任务的数目将数据划分为相同数目的分区，也就是一个reduce任务对应一个分区的数据。这样做是为了避免有些reduce任务分配到大量数据，而有些reduce任务却分到很少数据，甚至没有分到数据的尴尬局面。然后对每个分区中的数据进行排序，将排序后的结果进行合并操作，这样做可以有效减少磁盘IO和网络IO。

- 当map任务输出最后一个记录时，可能会有很多的溢出文件，这时需要将这些文件合并。合并的过程中会不断地进行排序和合并操作，这样做是为了尽量减少每次写入磁盘的数据量和尽量减少下一复制阶段网络传输的数据量。最后合并成了一个已分区且已排序的文件。为了减少网络传输的数据量，这里可以将数据压缩。

- 将分区中的数据拷贝给相对应的reduce任务。那么分区中的数据如何知道它对应的reduce是哪个呢？ 程序保存了整个作业的宏观信息，只要reduce任务向程序获取对应的map输出位置就可以了。

![1575006926888](image\1575006926888.png)

### 2.4 Reduce 步骤

- Reduce会接收到不同map任务传来的数据，并且每个map传来的数据都是有序的。如果reduce接受的数据量相当小，则直接存储在内存中，如果数据量超过了该缓冲区大小的一定比例，则对数据合并后溢写到磁盘中。

- 随着溢写文件的增多，后台线程会将它们合并成一个更大的有序文件，这样做是为了给后面的合并节省时间。其实不管在map端还是reduce端，Map Reduce都是反复地执行排序、合并操作，所以说排序是hadoop的灵魂。

- 在合并的过程中会产生许多的中间文件，但Map Reduce会让写入磁盘的数据尽可能地少，并且最后一次合并的结果并没有写入磁盘，而是直接输入到reduce函数。

![1575007205277](image\1575007205277.png)

### 2.5 词频统计

```Python
# map函数
import os
import sys
import re

if __name__ == '__main__':
    handler = sys.stdin
    for line in handler:
        if not line:
            continue
        terms = line.strip().split(" ")
        for i in terms:
            print(i)

        
# Reduce函数
import os
import sys
import re

if __name__ == '__main__':
    handler = sys.stdin
    word_dict = {}
    for line in handler:
        if not line:
            continue
        terms = line.strip().split(" ")
        for i in terms:
            if i in word_dict:
                word_dict[i] += 1
            else:
                word_dict[i] = 1

    for j in word_dict:
        print(j, word_dict[j])
```

![1575007915262](image\1575007915262.png)

### 2.5 Map Reduce并行化

- 将输入的海量数据切片分给不同的机器处理；

- 执行 Map 任务的 Worker 将输入数据解析成 key/value pair，用户定义的 Map 函数把输入的 key/value 转成中间形式的 key/value ；

- 按照 key 值对中间形式的 key/value 进行排序、聚合；

- 把不同的 key 值和相应的 value 集分配给不同的机器，完成 Reduce 运算；

按照Map Reduce的执行步骤，将海量的数据进行分割，然后传递给不同的map，这样就实现了并行化的操作。

![1575011080325](image\1575011080325.png)

### 2.6 Map Reduce环境

- 运行Map-Reduce模型，还需要hadoop环境解决
  - 对原始数据进行区分(Partition)
  - 调度程序在一系列的机器集群上都并行运行
  - 执行中间过程的group by key 步骤
  - 处理运行过程中的突发节点故障
  - 处理并行运行过程中的节点和节点之间的通信

### 2.7 数据流

数据流是一组有序，有起点和终点的字节的数据序列。包括输入流和输出流。数据流是一串连续不断的数据的集合，就象水管里的水流，在水管的一端一点一点地供水，而在水管的另一端看到的是一股连续不断的水流。数据写入程序可以是一段、一段地向数据流管道中写入数据，这些数据段会按先后顺序形成一个长的数据流。对数据读取程序来说，看不到数据流在写入时的分段情况，每次可以读取其中的任意长度的数据，但只能先读取前面的数据后，再读取后面的数据。不管写入时是将数据分多次写入，还是作为一个整体一次写入，读取时的效果都是完全一样的。 

- 输入和输出都被存储在分布式文件系统HDFS上
- 实际调度操作时，调度器会尽可能将map任务移至靠近数据物理存储的节点上
- 中间结果将会被存储在map和reduce操作的本地文件系统上
- 实际运行过程中，一个Map-Reduce产生的结构，很有可能作为另一个Map-Reduce任务的输入

### 2.8 Data Node和Name Node工作机制

NameNode：管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间；

DataNode：负责存储client发来的数据块block；执行数据块的读写操作。

Namenode是中心服务器，单一节点（简化系统的设计和实现），负责管理文件系统的名称空间（namespace）以及客户端对文件的访问。Namenode负责文件元数据的操作，DataNode负责处理文件内容的读写请求。Namenode全权管理数据块的复制，它周期性地从集群中的每个Datanode接收心跳信号和块状态报告。接收到心跳信号意味着该Datanode节点工作正常。块状态报告包含了一个该Datanode上所有数据块的列表。Namenode是主节点，存储文件的元数据如文件名，文件目录结构，文件属性（生成时间，副本数，文件权限）以及每个文件的块列表，以及块所在的DataNode等等。

![1575012777667](image\1575012777667.png)

### 2.9 节点

#### 2.9.1 主节点的协调功能

- Namenode ： 目录的管理者，每一个集群都有一个，记录实时的数据变化

- Datanode ：是文件系统的工作节点

- Secondarynode ：一个用来监控HDFS状态的辅助后台程序

- Resourcemanager ：与客户端进行交互，处理来自于客户端的请求

- Nodemanager ：NM是ResourceManager在每台机器上的代理，负责容器管理，并监控它们的资源使用情况
  - 主节点主要负责系统的协调
  - 任务状态：等待初始，进行中，完成
  - 一旦有能工作的worker，待初始任务被调度运行
  - 一个Map任务完成后，它会向主节点发送它的产生的R个中间文件	的位置和大小，每个文件对应一个reducer
  - 主节点将这些信息传送至reducer

#### 2.9.2 节点故障

- Map任务节点故障
  - 所有运行中和已经完成的map任务，都被重置为待初始
  - 所有这些待初始Map任务，将重新被分配到能工作的节点worker
- Reduce任务节点故障
  - 只有运行中而未完成的reduce任务被设定为待初始
  - 这些待初始reduce任务被重新分配至其他worker上
- 主节点故障
  - 整个Map-Reduce任务中断，同时通知客户端管理员

### 2.10 调优

#### 2.10.1 Map与Reduce

- 启动多少个Map和Reduce任务
  - M个Map任务和R个Reduce任务
  - 实际操作经验法则：
    	通常情况下我们会让M远大于集群中的节点数
    	通常设置一个分布式文件系统块对应一个Map任务
    	提升动态加载平衡，同时加速节点故障时的任务恢复
  - 通常R比M要小，因为输出要分布在R文件上

- 动态添加map和reduce的大小，增加并行度
  - map是配置mapred.max.split.size，来定义map处理文件的大小，默认是256000000字段，换算就是256M。 如果想增加map的并行度，那么就是减少map处理文件的大小 setmapred.max.split.size（更小的字节）
  - reduce和map是一致的，修改hive.exec.reducers.bytes.per.reducer这个参数 通过控制这个来定义一个reduce处理文件的大小 hive.exec.reducers.bytes.per.reducer

|                  属性                   |    默认值    |                             描述                             |
| :-------------------------------------: | :----------: | :----------------------------------------------------------: |
|               io.sort.mb                |     100      |              映射输出分类时所使用缓冲区的大小.               |
|         io.sort.record.percent          |     0.05     | 剩余空间用于映射输出自身记录.在1.X发布后去除此属性.随机代码用于使用映射所有内存并记录信息. |
|          io.sort.spill.percent          |     0.80     |        针对映射输出内存缓冲和记录索引的阈值使用比例.         |
|             io.sort.factor              |      10      | 文件分类时合并流的最大数量。此属性也用于reduce。通常把数字设为100. |
|       min.num.spills.for.combine        |      3       |                组合运行所需最小溢出文件数目.                 |
|       mapred.compress.map.output        |    false     |                        压缩映射输出.                         |
|   mapred.map.output.compression.codec   | DefaultCodec |                 映射输出所需的压缩解编码器.                  |
|      mapred.reduce.parallel.copies      |      5       |             用于向reducer传送映射输出的线程数目.             |
|       mapred.reduce.copy.backoff        |     300      | 时间的最大数量，以秒为单位，这段时间内若reducer失败则会反复尝试传输 |
|             io.sort.factor              |      10      |                组合运行所需最大溢出文件数目.                 |
| mapred.job.shuffle.input.buffer.percent |     0.70     |           随机复制阶段映射输出缓冲器的堆栈大小比例           |
|    mapred.job.shuffle.merge.percent     |     0.66     | 用于启动合并输出进程和磁盘传输的映射输出缓冲器的阀值使用比例 |
|      mapred.inmem.merge.threshold       |     1000     | 用于启动合并输出和磁盘传输进程的映射输出的阀值数目。小于等于0意味着没有门槛，而溢出行为由 mapred.job.shuffle.merge.percent单独管理. |
| mapred.job.reduce.input.buffer.percent  |     0.0      | 用于减少内存映射输出的堆栈大小比例，内存中映射大小不得超出此值。若reducer需要较少内存则可以提高该值. |

#### 2.10.2 combiners

- 很多时候一个Map任务为同一个key k会产生如（k,v1）,(k,v2)的键值对：
  - 例如，词频统计任务中的高频词产生的中间结果
- 在Mapper中，进行预聚合操作，来节约网络的时间成本
  - 合并（k, list(v1)）-> v2
  - 合并器(combiner)通常和reduce函数是一致的

- 合并器（Combiner）预先合并了单个mapper（单个节点）中键值对

![1575015490135](image\1575015490135.png)

### 2.11 总结

- 资源链接

  - [Hadoop Wiki](http：//wiki.apache.org/lucene-hadoop/)
  - [开始向导](http：//wiki.apache.org/lucene-hadoop/GettingStartedWithHadoop)

  - [Map/Reduce Overview](http：//wiki.apache.org/lucene-hadoop/HadoopMapReduce)
  - [Map/Reduce Overview](http：//wiki.apache.org/lucene-hadoop/HadoopMapRedClasses)

- reduce需要写函数，map有时候都不用写

- map工作主要修改key ，reduce主要修改values

- 对已有的算法进行map-reduce化

- map 对一个键值对输入产生一序列中间键值对

- map函数将对所有输入键值对操作

- 相同的key 值 v 被reduce放一起，Reduce函数对每一个不同的key进行操作

- map和reduce属于分治思想，通过hash分桶来处理，map是发散的过程，reduce是收敛的过程

- map任务数目要远大于Reduce

- map-reduce会有输入和输出，输出后再次进入map-reduce，如此循环迭代，在磁盘级别的操作，所以开销

- 会很大，spark是在内存级别的操作，所有对内存开销会很大，但速度很快

- spark稳定不如map，spark只读一次

- map-reduce主要做特征的转换，数据的提取，转换，处理，写入

- 做特征的用map，reduce，导出的特征用于机器学习训练的用spark建模， 用hadoop streaming方便任何语言编写map-reduce

## 3 HDFS

Hadoop分布式文件系统HDFS。它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分系统约束，来实现流式读取文件系统数据的目的。

### 3.1 HDFS 架构

![1575016212680](image\1575016212680.png)

- ##### Block数据块

  基本存储单位，一般大小为64M，一般硬盘传输速率比寻道时间要快，大的块可以减少寻道时间；减少管理块的数据开销，每个块都需要在NameNode上有对应的记录，对数据块进行读写，减少建立网络的连接成本

- ##### NameNode

  运目录的管理者，每一个集群都有一个，记录实时的数据变化，NameNode失效则整个HDFS都失效了，所以要保证NameNode的可用性

- ##### Secondary NameNode

  定时与NameNode进行同步（定期合并文件系统镜像和编辑日志，然后把合并后的传给NameNode，替换其镜像，并清空编辑日志，类似于CheckPoint机制），但NameNode失效后仍需要手工将其设置成主机

- ##### DataNode

  保存具体的block数据，负责数据的读写操作和复制操作，DataNode启动时会向NameNode报告当前存储的数据块信息，后续也会定时报告修改信息，DataNode之间会进行通信，复制数据块，保证数据的冗余性

### 3.2 HDFS 读取原理

- #### 写文件

  - 客户端将文件写入本地磁盘的HDFS Client文件中
  - 当临时文件大小达到一个block大小时，HDFS client通知NameNode，申请写入文件
  - NameNode在HDFS的文件系统中创建一个文件，并把该block id和要写入的DataNode的列表返回给客户端
  - 客户端收到这些信息后，将临时文件写入DataNodes
  - 文件写完后（客户端关闭），NameNode提交文件

- #### 读文件
  - 客户端向NameNode发送读取请求
  - NameNode返回文件的所有block和这些block所在的DataNodes（包括复制节点）
  - 客户端直接从DataNode中读取数据，如果该DataNode读取失败，则从复制节点中读取

### 3.3 HDFS 总结

- #### HDFS优点总结：

  - 支持任意超大文件存储；硬件节点可不断扩展，低成本存储（真实案例为：4000节点，目前最大5000节点）；
  - 对上层应用屏蔽分布式部署结构，提供统一的文件系统访问接口，感觉就是一个大硬盘；应用无需知道文件具体存放位置，使用简单；
  - 文件分块存储（1块缺省64MB)，不同块可分布在不同机器节点上，通过元数据记录文件块位置；应用顺序读取各个块；
  - 系统设计为高容错性，允许廉价PC故障；每块文件数据在不同机器节点上保存3份；这种备份的另一个好处是可方便不同应用就近读取，提高访问效率。

- #### HDFS缺点总结：

  - 适合大数据文件保存和分析，不适合小文件，由于分布存储需要从不同节点读取数据，效率反而没有集中存储高；一次写入多次读取，不支持文件修改；
  - 是最基础的大数据技术，基于文件系统层面提供文件访问能力，不如数据库技术强大，但也是海量数据库技术的底层依托；
  - 文件系统接口完全不同于传统文件系统，应用需要重新开发。

## 4 YARN

### 4.1 YARN架构概述

YARN 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操 作系统平台，而 Map Reduce 等运算程序则相当于运行于操作系统之上的应用程序。

MapReduce 是 YARN 的一个特例。 YARN 则是 Map Reduce 的一个更加通用和高级的框架形式，并在其上增加了更多的功能。例如通过加载分布式执行脚本可以在集群节点上执行独立的脚本任务。所以我们可以看到，YARN 可以直接运行在 Map Reduce 运行的框架上而不会造成更多的干扰，并且会为集群的运算带来更多的好处。更一步的开发显示了 YARN 会允许开发者根据自己的需求运行不同版本的 MapReduce 在集群中，这将为开发者提供更为便捷的服务。
YARN 是 Hadoop2.x 版本中的一个新特性。它的出现其实是为了解决第一代 MapReduce 编程 框架的不足，提高集群环境下的资源利用率，这些资源包括内存，磁盘，网络，IO等。Hadoop2.X 版本中重新设计的这个 YARN 集群，具有更好的扩展性，可用性，可靠性，向后兼容性，以 及能支持除 MapReduce 以外的更多分布式计算程序

- YARN 并不清楚用户提交的程序的运行机制

- YARN 只提供运算资源的调度（用户程序向 YARN 申请资源，YARN 就负责分配资源）

- YARN 中的主管角色叫 ResourceManager

- YARN 中具体提供运算资源的角色叫 NodeManager

- YARN 其实与运行的用户程序完全解耦，就意味着YARN上可以运行各种类型的分布式运算程序，MapReduce 只是其中的一种，Spark、Storm 、flink等运算框架都可以整合在 YARN 上运行，只要他们各自的框架中有 符合 YARN 规范的资源请求机制即可。

- YARN 就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整 合在一个物理集群上，提高资源利用率，方便数据共享

- #### 任务过程

  - 首先是任务的提交。对于任务的提交，与 MapReduce 相类似，客户端向 MapReduce 提交任务。在提交任务后，返回一个新的可供运行的应用管理器对其进行管理。

  - ResourcesManager返回一个新的Job ID。

  - Job 客户端计算输入分片，拷贝资源(包括 Job JAR文件、配置文件，分片信息)到 HDFS。

  - 用SubmitApplication 方法提交Job 给 MapReduce 进行处理。

  - 任务提交申请资源调用

### 4.2 YARN组成

- #### ResourceManager（RM）

YARN分层结构的本质是ResourceManager。这个实体控制整个集群并管理应用程序向基础计算资源的分配。ResourceManager将各个资源部分（计算、内存、带宽等）安排给NodeManager（YARN的每节点代理）。

1. 处理客户端请求

2. 启动或监控ApplicationMaster

3. 监控NodeManager

4. 资源的分配与调度

- #### ApplicationMaster（AM）

ApplicationMaster管理在YARN内运行的每个应用程序实例。ApplicationMaster负责协调来自ResourceManager的资源，并通过NodeManager监视容器的执行和资源使用（CPU、内存等的资源分配）。

1. 计算应用的资源需求，

2. 根据数据来申请对应位置的资源

3. 向ResourceManager申请资源，与NodeManager交互进行程序的运行和监控，监控申请的资源的使用情况，监控作业进度

4. 跟踪任务状态和进度，定时向ResourceManager发送心跳消息，报告资源的使用情况和应用的进度信息

5. 负责本作业内的任务的容错

- #### NodeManager（NM）

NodeManager管理YARN集群中的每个节点。NodeManager提供针对集群中每个节点的服务，从监督对一个容器的终生管理到监视资源和跟踪节点健康。

1. 启动时向ResourceManager注册并定时发送心跳消息，等待ResourceManager的指令

2. 监控Container的运行，维护Container的生命周期，监控Container的资源使用情况

3. 启动或停止Container，管理任务运行时的依赖包

- #### Container

Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示的。

1. 基本的资源单位（CPU、内存等）
2. Container可以加载任意程序，而且不限于Java
3. 一个Node可以包含多个Container，也可以是一个大的Container
4. ApplicationMaster可以根据需要，动态申请和释放Container

## 5 Hive

### 5.1 Hive与Spark介绍

- #### Hive 

在Hadoop的Map-Reduce之上提供的类SQL数据提取操作功能。 Hive是基于 Hadoop 的一个【数据仓库工具】，可以将结构化的数据文件映射为一张数据库表，并提供简单的 sql 查询功能，可以将 sql 语句转换为 MapReduce
任务进行运行。使用SQL来快速实现简单的MapReduce 统计，不必开发专门的MapReduce 应用，学习成本低，十分适合数据仓库的统计分析。

- #### Spark

Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架，Spark提供了一个全面、统一的框架用于管理各种有着不同性质（文本数据、图表数据等）的数据集和数据源（批量数据或实时的流数据）的大数据处理的需求，是Map-Reduce替代方案，兼容HDFS和Hive，可兼容hadoop生态，弥补Mapduce不足

### 5.2 Hive结构













### 5.3 Spark框架

















## 6 关联规则挖掘

### 6.1 关联规则介绍







### 6.2 支持度与信任度







### 6.3 强关联规则







### 6.4 关联规则算法



- ##### Apriori 

Apriori算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。通过该算法我们可以对数据集做关联分析——在大规模的数据中寻找有趣关系的任务频繁项集【经常出现在一块的物品的集合】、关联规则【暗示两种物品之间可能存在很强的关系】。

- 优点
  - 适合稀疏数据集。
  - 算法原理简单，易实现。
  - 适合事务数据库的关联规则挖掘。

- 缺点
  - 可能产生庞大的候选集。
  - 算法需多次遍历数据集，算法效率低，耗时。

- ##### Eclat

Eclat 垂直数据格式有效挖掘频繁项集，Eclat算法最大的特点便是**倒排思想**，也就是生成一个统计每一个项在哪些事务中出现过的倒排表，表中的每一行由项和它对应的TID集组成，TID集即包含此项目的所有事务的集合。

Eclat算法挖掘频繁项集的过程如下：

- 通过扫描一次数据集，把水平格式的数据转换成垂直格式；

- 项集的支持度计数简单地等于项集的TID集的长度；

- 从k=1开始，可以根据先验性质，使用频繁k项集来构造候选（k+1）项集；

- 通过取频繁k项集的TID集的交，计算对应的（k+1）项集的TID集。

- 重复该过程，每次k增加1，直到不能再找到频繁项集或候选项集